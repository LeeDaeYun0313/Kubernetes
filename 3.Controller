1-!!Controller!!!!!!!!!!!!!!!!

rc는 label을 기준으로 동작함 진짜중요! 별별별별

rc(ReplicationController)


apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-nginx
spec:         // 이거 3개 띄워줘
  replicas: 3 //
  selector:   // 
    app: web  
  template:
    metadata:
      name: nginx-pod
      labels:
        app: web
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.14

지금현재 rc가 label이 app: web인것을 3개 띄울려고 고정시키는데 내가 자의로 지워도 다시 자동으로 생성됩니다 3개로 맞춰서
만약 내가 다른 yaml파일에 label 에 app: web 을 넣어서 실행시키면 실행하는순간 바로죽어버린다 왜냐하면 rc가 label 이 app: web 인것을 3개만 띄우려고 명시해놔서

또,내가 nginx웹서버 버전을 edit로 바꾸면 안바뀐다 왜냐하면 rc는 label만봐서 다른건 신경안쓴다. 하지만 현재 3개가 띄워져있고 하나를 삭제하면
하나가 다시 뜰때는 바꾼 버전으로 pod가 실행된다.

kubectl get rc //로 rc확인
kubectl describe rc rc-nginx //자세히 rc확인

현재 rc 가 3개실행중일때  rc이름은 re-nginx로 했을때
kubectl scale rc rc-nginx --replicas=2 //하면 pod가 2개가됨

kubectl edit rc rc-nginx //해서 replicas: 5 로 수정해주면 5개로 바뀜

당연한말이지만 kubectl delete pod --all //하면 삭제되었다가 바로살아남 왜냐하면 rc는 rc가 관리하기때문임
kubectl delete rc rc-nginx //해야 깨끗이 지워짐 


2-!!ReplicaSet!!!!!!!!!!!!!!!!!
RS rs

ReplicaSet(rs) 은 rc보다 풍부한 selector 제공  

selector:
  matchLabels:
    component: redis
  matchExpressions:
    -{key:tier,operator: In, values:[cache]}
    -{key:environment,operator:NotIn,values:[dev]}
    
matchExpressions 연산자
 1.In: key와 value를 지정하여 key,value가 일치하는 pod만 연결
 2.Exists: key에 맞는 label의 pod를 연결
 3.NotIn
 4.DoesNotExist

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡrs-nginx.yamlㅡㅡㅡㅡㅡㅡㅡㅡㅡ
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-nginx
spec:
  replicas: 3
  selector:
    matchLabels:      // rc와다르게 rs는 matchLabels를 사용 하고 그밑에 app: 을넣음
      app: web
    matchExpressions:
    - {key: version, operator: In, values: ["1.14", "1.15"]}        // rc와 다르게 rs는 다른버젼을 여러개 생성가능 !! 
  template:
    metadata:
      name: nginx-pod
      labels:
        app: web
        version: "1.14"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.14
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
rs는 rc의 상위호환 느낌

!!3-Deployment 
디플로이가 rs만들고 rs가 pod를 만든는것

rs위에서 행동함 
처음에 내가 nginx 1.14 3개생성하면 delpoyment 밑에 rs가 생성되서 rs밑에 pod3개가 생성됨 nginx-1.14버젼
그런후 내가 ngunx 1.15 버젼으로 업데이트 하라고하면 Rolling update를함 Rolling update는 첫번째 rs의 pod를 하나죽이고
두번째 rs를 생성해 1.15버젼으로 pod를 하나 생성하고 그후 첫번째 rs의 2번째 pod를 삭제하고 두번째 rs에 1.15버전 pod를 하나생성
그런식으로 계속 진행

그러니까 yaml파일이 rs와 deployment는 다를게 없다 kind는 종류니까 당연히 다른것이고~

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡdeploy-nginx.yamlmㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rs-nginx
spec:
  replicas: 3
  selector:
    matchLabels:      
      app: web
    matchExpressions:
    - {key: version, operator: In, values: ["1.14", "1.15"]}        
  template:
    metadata:
      name: nginx-pod
      labels:
        app: web
        version: "1.14"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.14
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
kubectl apply -f deploy-nginx.yaml //하면 pod 3개가 띄워지고
kubectl set image deployment deploy-nginx nginx-container=nginx:1.15 --record //하면 1.14버전이 1.15버전으로 롤링업데이트된다

kubectl rollout history deployment deploy-nginx //보면 처음만들었던기록, 두번째 업데이트했던기록 3번째 업데이트했던 기록이 나온다.

kubectl rollout undo deployment deploy-nginx //이렇게 하면 바로 전단계로 돌아간다
kubectl rollout undo deployment deploy-nginx --to-revision 2 //하면 2번째 기록되었던 버전으로 돌아간다.

kubectl get rs //해보면 사용중이지않은rs들(이전 버젼의 pod 가지고있던 rs)이 나오고 현재 사용중인 rs들이 나온다.

  //Rolling update
    kubectl set image deployment <deploy_name> <container_name>=<new_version_image>
  //RollBack
    kubectl rollout history deployment <deploy_name>
    kubectl rollout undo deploy <deploy_name>

Annotation방법 record 상위호환
Annotation
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡdeploy-update.yamlㅡㅡㅡㅡㅡ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-nginx-update
  annotations:                                // 여기에 히스토리가 담긴다
    kubernetes.io/change-cause: version 1.14  // 
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      name: nginx-pod
      labels:
        app: web
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.14                       //여기 이미지 버전을 바꿔준다 
 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
 
 처음에 1.14버전을 설치하고  그후 vi 로 들어가서 annotations랑 image를 1.15로 바꿔주고 kubectl apply -f 디플로이 실행 시키면 히스토리도 남고 롤링업데이트되서 1.1.5버전으로 바뀜

 kubectl rollout history deployment deploy-nginx-update(디플로이이름) //해서 히스토리 버전 확인가능
 
 kubectl rollout undo deployment deploy-nginx-update(디플로이이름) //하면 전의 버전으로 돌아감
 
 kubectl rollout undo deployment deploy-nginx-update --to-revision 1 //하면 1번째 revision의 히스토리로 돌아감 

---------DaemonSet-------------------------------------------

DaemonSet  //현재 노드당 1개씩만 실행된다. ex)모니터링 에이전트 , 위브웍스 , 로그수집, 하드웨어 모니터링 등등 반드시 노드에 1개있으면서 무엇인가를 책임져야하는거 꼭 필요한 것.

kubectl get pods -A // 숨겨진 pod 모두 보기
kubectl get daemonsets.apps -A //데몬셋 돌아가는 pod 전체보기

ㅡㅡㅡㅡㅡㅡㅡㅡㅡdaemonset-nginx.yamlㅡㅡㅡㅡㅡㅡㅡㅡㅡ
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ds-nginx
spec:
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      name: nginx-pod
      labels:
        app: web
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.14
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

rs랑 비슷하게 selector matchlabels 들어간다 같음

데몬셋은 노드하나 추가되면 데몬셋 자동실행되고 삭제하면 자동 삭제한다.
당연하게 파드만 삭제하면 다시 실행된다. 삭제하려면 데몬셋을 삭제 해야한다.
kubectl apply -f deamonset-nginx.yaml

kubectl get daemonset.apps
kubectl get pods

kubectl edit daemonset  ds-nginx //하고 nginx를 1.15버전으로 바꾸면 롤링업데이트가 자동으로된다.
kubectl rollout undo daemonset ds-nginx //하면 롤링백 된다. 전에 했던 버전으로 돌아감

----------StatefulSet------------------------------

StatefulSet //은 파드의 이름을 보장해준다 ex) stabc-0, stabc-1,stabc-2
// pod의 상태를 유지해주는 컨트롤러, pod이름, pod의 볼륨  
pod는 고유한 id를 가지고 순서대로 실행된다.
pod의 지속적인 상태를 유지
 1.pod name
 2.host name
 3.storage volume
 

ㅡㅡㅡㅡㅡㅡst-nginx.yamlㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: st-nginx
spec:
  replicas: 3
  serviceName: st-nginx-svc
  selector:
    matchLabels:
      app: web
    matchExpressions:
    - {key: version, operator: In, values: ["1.14", "1.15"]}
  template:
    metadata:
      name: nginx-pod
      labels:
        app: web
        version: "1.14"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.14

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

kubectl apply -f rs-nginx.yaml  -f st-nginx.yaml //두개 실행
kubectl edit statefulsets.apps st-nginx //1.15로 변경 nginx
kubectl delete statefulsets.apps st-nginx //삭제
kubectl delete replicasets.apps rs-nginx //삭제

------------job---------------------------------------------------------------------------------------

batch 처리하는 pod는 작업이 완료되면 종료됨.
  작업이 완료되어도 pod가 제거되지는 않음
  작업이 완료되는 시점이 중요한 서비스에 운용

batch 처리에 적합한 컨트롤러로 pod의 성공적인 완료를 보장
  pod 내의 컨테이너가 비정상 종료시 다시 실행
  pod내의 컨테이너가 정상 종료시 완료


job은 실행후 끝이나면 다시 실행되지않고 컴플리트 상태에 머문다. 
job은 삭제되지않고 그대로 있는다 왜냐하면 로그정보 등등 확인을 위해서

ㅡㅡㅡㅡㅡㅡㅡㅡㅡjob-example.yamlㅡㅡㅡㅡㅡㅡ
apiVersion: batch/v1
kind: Job
metadata:
  name: job-example
spec:
  template:
    spec:
      containers:
      - name: centos-container
        image: centos:7
        command: ["bash"]
        args:
        - "-c"
        - "echo 'Hello World'; sleep 50; echo 'Bye'"
      restartPolicy: Never

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

restartPolicy:
   -OnFailure: 비정상 종료시 원래 실행 중 이던 노드에서 컨테이너 재시작
   -Never : 비정상 종료 시 재시작을 막은 후 새로운 파드실행
Parallel Jobs
   -completions: Job 오브젝트로 실행할 pod 수
   -parallelism: job에서 여러개의 pod를 동시에 실행

위에 yaml 레이블은//spec: 안에 사용하면 된다


yaml파일 실행하고나서 pod만 지우면 pod는 지워지는데 job컨트롤러는 동작중이다. 당연하게 컨트롤러를 삭제해야한다.


kubectl get job //잡 확인
kubectl delete job job-example //잡 컨트롤러 삭제


--------cronjob----------------------------------------------------------

job의 상위 
job 오브젝트에 linux cronjob 의 스케중링 기능을 추가
다음과 같은 자동화 작업에 유리
   1.data backup
   2.send email
   3.cleaning tasks
cronjob controller가 관리

Cronjob Schedule: "0 3 1 * *" 0분 3시 1일 매월 요일전부 => 매월 1일 3시에

분  시  일  월  요일

0   5   *   *    0     --> 5시정각 일요일에 이 job을 실행

* =all

"*/5 * * * *" --> 5분마다 한 번씩 반복해서 실행

"0  5  *  *  6" --> 매주 토요일 새벽 5시에 반복해서 실행

ㅡㅡㅡㅡㅡㅡㅡㅡcronjon-example.yamlㅡㅡㅡㅡㅡㅡㅡㅡㅡ
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-example
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command:
            - /bin/sh
            - -c
            - date; echo Hello
          restartPolicy: OnFailure

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
job에 schedule: "*/1 * * * *"
        jobTemplate:
        spec:
3줄 추가

실행하게되면 1분마다 자동으로 실행된다. job과 마찬가지로 pod가 완료되면 컴플리트 상태로 남아있다. 많이 남아있을 것이다.

kubectl edit cronjobs.batch cronjob-example //해서 보면 limit제한이 있는데 3개라고 되어있을 것이다 그래서 최근 pod 3개만 남아있는다


